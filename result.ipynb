{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tensorflow请使用2.15版本\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Flatten, concatenate, LSTM, Dropout, Embedding\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "train_data = pd.read_csv(\"train.txt\")\n",
    "test_data = pd.read_csv(\"test_without_label.txt\")\n",
    "\n",
    "# 把标签转化为数值\n",
    "dic = {'negative':0, 'neutral':1, 'positive':2}\n",
    "train_data['tag'] = train_data['tag'].map(dic)\n",
    "train_data.head()\n",
    "\n",
    "# 分离图片和文字数据\n",
    "def read_img(id):\n",
    "  img = image.load_img('data/'+ str(id) + '.jpg',target_size=(224,224,3))\n",
    "  img = image.img_to_array(img)\n",
    "  img = img/255\n",
    "  return img\n",
    "\n",
    "def read_txt(id):\n",
    "    with open('data/'+ str(id)+ '.txt', 'r', encoding='GBK', errors='ignore') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "    return data\n",
    "\n",
    "train_data['text'] = train_data['guid'].apply(read_txt)\n",
    "train_data['img'] = train_data['guid'].apply(read_img)\n",
    "\n",
    "# 分割训练集验证集\n",
    "train, val = train_test_split(train_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>2845</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @orrie_yes: Need to calm myself so here's o...</td>\n",
       "      <td>[[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>430</td>\n",
       "      <td>0</td>\n",
       "      <td>#ANIMALABUSE #TORONTO #PUPPY #TORTURE WE OFFER...</td>\n",
       "      <td>[[[0.96862745, 0.96862745, 0.96862745], [0.984...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>4392</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @WorIdStarComedy: #TodaysKidsWillNeverKnow ...</td>\n",
       "      <td>[[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>4012</td>\n",
       "      <td>2</td>\n",
       "      <td>Thank u for your understanding heart and shini...</td>\n",
       "      <td>[[[0.8156863, 0.67058825, 0.49411765], [0.8039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>2379</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @theIeansquad: When you rob a black persons...</td>\n",
       "      <td>[[[0.07058824, 0.07058824, 0.07058824], [0.070...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>4193</td>\n",
       "      <td>2</td>\n",
       "      <td>Euro buoyant ahead of Greek vote http://t.co/q...</td>\n",
       "      <td>[[[0.72156864, 0.6784314, 0.43529412], [0.7254...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "      <td>#trashcomics lmao dick, so incensed. HOW COULD...</td>\n",
       "      <td>[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>1652</td>\n",
       "      <td>2</td>\n",
       "      <td>#February #Winter #Rainy #Stormy #Windy #Tuesd...</td>\n",
       "      <td>[[[0.4, 0.38039216, 0.30588236], [0.3764706, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>2609</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @neiltyson: I once showed Pluto to Pluto. H...</td>\n",
       "      <td>[[[0.39215687, 0.4392157, 0.29803923], [0.2431...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3804</th>\n",
       "      <td>4146</td>\n",
       "      <td>0</td>\n",
       "      <td>1100/45R46 BLEMISHED R1-W LSW 177A/8 TIRE http...</td>\n",
       "      <td>[[[0.23529412, 0.23137255, 0.22352941], [0.219...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      guid  tag                                               text  \\\n",
       "2595  2845    2  RT @orrie_yes: Need to calm myself so here's o...   \n",
       "2982   430    0  #ANIMALABUSE #TORONTO #PUPPY #TORTURE WE OFFER...   \n",
       "246   4392    2  RT @WorIdStarComedy: #TodaysKidsWillNeverKnow ...   \n",
       "862   4012    2  Thank u for your understanding heart and shini...   \n",
       "1941  2379    1  RT @theIeansquad: When you rob a black persons...   \n",
       "...    ...  ...                                                ...   \n",
       "1557  4193    2  Euro buoyant ahead of Greek vote http://t.co/q...   \n",
       "2997   333    0  #trashcomics lmao dick, so incensed. HOW COULD...   \n",
       "3004  1652    2  #February #Winter #Rainy #Stormy #Windy #Tuesd...   \n",
       "3687  2609    2  RT @neiltyson: I once showed Pluto to Pluto. H...   \n",
       "3804  4146    0  1100/45R46 BLEMISHED R1-W LSW 177A/8 TIRE http...   \n",
       "\n",
       "                                                    img  \n",
       "2595  [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0,...  \n",
       "2982  [[[0.96862745, 0.96862745, 0.96862745], [0.984...  \n",
       "246   [[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0,...  \n",
       "862   [[[0.8156863, 0.67058825, 0.49411765], [0.8039...  \n",
       "1941  [[[0.07058824, 0.07058824, 0.07058824], [0.070...  \n",
       "...                                                 ...  \n",
       "1557  [[[0.72156864, 0.6784314, 0.43529412], [0.7254...  \n",
       "2997  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
       "3004  [[[0.4, 0.38039216, 0.30588236], [0.3764706, 0...  \n",
       "3687  [[[0.39215687, 0.4392157, 0.29803923], [0.2431...  \n",
       "3804  [[[0.23529412, 0.23137255, 0.22352941], [0.219...  \n",
       "\n",
       "[3200 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 19s 0us/step\n"
     ]
    }
   ],
   "source": [
    "## 图片处理部分\n",
    "input_image = Input(shape=(224,224,3))\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_image)\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "output_image = Dense(128, activation='relu')(x)\n",
    "\n",
    "## 文字处理部分\n",
    "# 可以通过调用Transformer库中的开源模型生成更好的embedding\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(train['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_X = tokenizer.texts_to_sequences(train['text'].values)\n",
    "val_X = tokenizer.texts_to_sequences(val['text'].values)\n",
    "\n",
    "train_X = pad_sequences(train_X, maxlen=500)\n",
    "val_X = pad_sequences(val_X, maxlen=500)\n",
    "\n",
    "Y = pd.get_dummies(train['tag']).values\n",
    "X_img = np.array(train['img'].tolist())\n",
    "val_X_img = np.array(val['img'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "100/100 [==============================] - 741s 7s/step - loss: 0.9991 - accuracy: 0.5844 - val_loss: 6.9226 - val_accuracy: 0.5863\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 627s 6s/step - loss: 0.7459 - accuracy: 0.7241 - val_loss: 0.8695 - val_accuracy: 0.6275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x4371aa740>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型1\n",
    "input_text = Input(shape=(500, ))\n",
    "emb_text = Embedding(len(word_index) + 1, 100)(input_text)\n",
    "lstm_out = LSTM(300,dropout=0.2, recurrent_dropout=0.2,return_sequences=True)(emb_text)\n",
    "x = Flatten()(lstm_out)\n",
    "x = Dense(200,activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(100,activation='relu')(x)\n",
    "\n",
    "merge = concatenate([x, output_image])\n",
    "output = Dense(3,activation='softmax')(merge)\n",
    "\n",
    "model = Model(inputs=[input_text, input_image],outputs =output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "val_Y = pd.get_dummies(val['tag']).values\n",
    "\n",
    "model.fit([train_X, X_img], Y, validation_data=([val_X, val_X_img], val_Y), epochs=2, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 模型2（更复杂，训练起来更慢，供参考）\n",
    "# from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Flatten, BatchNormalization\n",
    "# from keras.models import Model\n",
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# input_text = Input(shape=(500, ))\n",
    "# emb_text = Embedding(len(word_index) + 1, 100)(input_text)\n",
    "# lstm_out = LSTM(300, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb_text)\n",
    "# lstm_out = Dropout(0.5)(lstm_out)  # 提高Dropout比例\n",
    "# lstm_out = BatchNormalization()(lstm_out)  # 添加批归一化层\n",
    "# flat_text = Flatten()(lstm_out)\n",
    "# dense_text_1 = Dense(200, activation='relu', kernel_regularizer='l2')(flat_text)  # 添加L2正则化\n",
    "# dense_text_1 = Dropout(0.5)(dense_text_1)  # 提高Dropout比例\n",
    "# dense_text_2 = Dense(100, activation='relu', kernel_regularizer='l2')(dense_text_1)  # 添加L2正则化\n",
    "\n",
    "# merge = concatenate([dense_text_2, output_image])\n",
    "# output = Dense(3, activation='softmax')(merge)\n",
    "\n",
    "# model = Model(inputs=[input_text, input_image], outputs=output)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # 添加早停法，防止过拟合\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# # 使用ReduceLROnPlateau来减少学习率，当指标停止提升时\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1, min_lr=1e-6)\n",
    "\n",
    "# val_Y = pd.get_dummies(val['tag']).values\n",
    "\n",
    "# # 添加callbacks到fit函数\n",
    "# model.fit(\n",
    "#     [train_X, X_img], Y,\n",
    "#     validation_data=([val_X, val_X_img], val_Y),\n",
    "#     epochs=3,\n",
    "#     batch_size=32,\n",
    "#     verbose=1,\n",
    "#     callbacks=[early_stopping, reduce_lr]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取测试数据集\n",
    "test_data = pd.read_csv('test_without_label.txt')\n",
    "\n",
    "test_data['text'] = test_data['guid'].apply(read_txt)\n",
    "test_data['img'] = test_data['guid'].apply(read_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 17s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# 文本和图片的预处理\n",
    "test_txt = tokenizer.texts_to_sequences(test_data['text'].values)\n",
    "test_txt = pad_sequences(test_txt, 500)\n",
    "test_img = np.array(test_data['img'].tolist())\n",
    "\n",
    "# 使用模型进行预测\n",
    "pred = model.predict([test_txt, test_img])\n",
    "labels = np.argmax(pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成标签\n",
    "dic_reverse = {v: k for k, v in dic.items()}\n",
    "labels = [dic_reverse.get(x) for x in list(np.argmax(pred, axis=-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入预测结果\n",
    "with open(\"test_with_label.txt\", \"w\") as outfile:\n",
    "    outfile.write('guid,tag\\n')\n",
    "    for guid, label in zip(test_data['guid'], labels):\n",
    "        outfile.write('{},{}\\n'.format(guid, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
